{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Feature Processing\n",
    "\n",
    "Dataset is first loaded into dictionary. Subsequently, data is explored (no. of persons in dataset, features with/without useful information, visualisation of features etc.) with the goal of understanding the data better and selecting the optimal features to use for training the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The general structure of the dataset is as follows:\n",
      "\n",
      "('METTS MARK', {'salary': 365788, 'to_messages': 807, 'deferral_payments': 'NaN', 'total_payments': 1061827, 'exercised_stock_options': 'NaN', 'bonus': 600000, 'restricted_stock': 585062, 'shared_receipt_with_poi': 702, 'restricted_stock_deferred': 'NaN', 'total_stock_value': 585062, 'expenses': 94299, 'loan_advances': 'NaN', 'from_messages': 29, 'other': 1740, 'from_this_person_to_poi': 1, 'poi': False, 'director_fees': 'NaN', 'deferred_income': 'NaN', 'long_term_incentive': 'NaN', 'email_address': 'mark.metts@enron.com', 'from_poi_to_this_person': 38})\n",
      "\n",
      "\n",
      "Total number of persons in dataset: 145 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structure of dataset (print out a key, value pair from data_dict)\n",
    "print \"The general structure of the dataset is as follows:\\n\"\n",
    "print (data_dict.items()[0])\n",
    "print \"\\n\"\n",
    "\n",
    "### Key Information\n",
    "\n",
    "# Removing outlier (Udacity requirement - Task 2)\n",
    "# During some preliminary analysis and data visualisation, an unnecessary data point, \"TOTAL\", was found to be \n",
    "# present. As this is actually a sum of all other persons in dataset, its not useful here and is removed.\n",
    "data_dict.pop('TOTAL', None)\n",
    "\n",
    "# No. of persons in dataest\n",
    "print \"Total number of persons in dataset:\", len(data_dict), \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of data pts available for each features (descending order):\n",
      "\n",
      "[('poi', 145), ('total_stock_value', 125), ('total_payments', 124), ('email_address', 111), ('restricted_stock', 109), ('exercised_stock_options', 101), ('salary', 94), ('expenses', 94), ('other', 92), ('to_messages', 86), ('shared_receipt_with_poi', 86), ('from_messages', 86), ('from_poi_to_this_person', 86), ('from_this_person_to_poi', 86), ('bonus', 81), ('long_term_incentive', 65), ('deferred_income', 48), ('deferral_payments', 38), ('restricted_stock_deferred', 17), ('director_fees', 16), ('loan_advances', 3)]\n"
     ]
    }
   ],
   "source": [
    "# Find how many data pts are available for each feature. \n",
    "# Since there are 145 persons in total, the max number of data pts possible for a feature is 145. \n",
    "# If number is too low (ie. feature is mostly NaN), it would probably not be ideal to use that feature for training.\n",
    "\n",
    "available_features_list = (data_dict.items()[0][1]).keys()\n",
    "feature_count = {}\n",
    "\n",
    "for feature in available_features_list:\n",
    "    count = 0\n",
    "    for values in data_dict.values():\n",
    "        if values[feature] != \"NaN\":\n",
    "            count += 1\n",
    "    feature_count[feature] = count\n",
    "\n",
    "sorted_feature_count = sorted(feature_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print \"No. of data pts available for each features (descending order):\\n\"\n",
    "print (sorted_feature_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some logical intuition, the following was done:\n",
    "- any feature for which less than half the total no. of persons had data, was removed from consideration\n",
    "- \"email_address\", \"other\" features were removed as they aren't too useful\n",
    "- \"poi\" removed since its used as label\n",
    "\n",
    "Resulting list shown below are the features to be considered for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total_stock_value', 'total_payments', 'restricted_stock', 'exercised_stock_options', 'salary', 'expenses', 'to_messages', 'shared_receipt_with_poi', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', 'bonus']\n",
      "12 features\n"
     ]
    }
   ],
   "source": [
    "cleaned_feature_list = []\n",
    "\n",
    "for element in sorted_feature_count:\n",
    "    if element[1]>=(145/2) and element[0]!=\"poi\" and element[0]!=\"email_address\" and element[0]!=\"other\":\n",
    "        cleaned_feature_list.append(element[0])\n",
    "\n",
    "print cleaned_feature_list\n",
    "print len(cleaned_feature_list), \"features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creating/processing new features</h3>\n",
    "\n",
    "As part of Udacity's requirements for the final project, new features were to be implemented based on the understanding of the data, and its strength compared to other features had to be considered when selecting features for training the final algoritm.\n",
    "\n",
    "The email features were explored for this requirement and new features were implemented as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAELCAYAAAAspXpuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111ad3e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initial visualization of email features to identify possible trends\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    from_poi = value[\"from_poi_to_this_person\"]\n",
    "    to_poi = value[\"from_this_person_to_poi\"]\n",
    "    if value[\"poi\"]==1:\n",
    "        plt.scatter(from_poi, to_poi, color=\"r\")\n",
    "    if value[\"poi\"]==0:\n",
    "        plt.scatter(from_poi, to_poi, color=\"b\")\n",
    "\n",
    "plt.xlabel(\"from_poi\")\n",
    "plt.ylabel(\"to_poi\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to not displaying any strong trend, this plot is not exactly a good representation of the features across the persons as the data points are not normalised to the number of emails sent or received by each person. \n",
    "\n",
    "Consequently, for each person, the fraction of emails sent/received that were to/from POI was calculated and then added to dataset. The data was also visualised as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAELCAYAAADdriHjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwGBmZhkODGZmluHAYGZmGf8fY8CTD4M+/QoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111ad3bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_fraction(A,B):\n",
    "    if A == \"NaN\" or B == \"NaN\":\n",
    "        return 0\n",
    "\n",
    "    return (1.0*A)/B\n",
    "\n",
    "for key, value in data_dict.items():\n",
    "    # compute fraction\n",
    "    from_poi_frac = compute_fraction(value[\"from_poi_to_this_person\"], value[\"to_messages\"])\n",
    "    to_poi_frac = compute_fraction(value[\"from_this_person_to_poi\"], value[\"from_messages\"])\n",
    "    \n",
    "    # store in data_dict\n",
    "    value[\"from_poi_frac\"] = from_poi_frac\n",
    "    value[\"to_poi_frac\"] = to_poi_frac\n",
    "    \n",
    "    if value[\"poi\"]==1:\n",
    "        plt.scatter(from_poi_frac, to_poi_frac, color=\"r\")\n",
    "    if value[\"poi\"]==0:\n",
    "        plt.scatter(from_poi_frac, to_poi_frac, color=\"b\")\n",
    "    \n",
    "plt.xlabel(\"from_poi_frac\")\n",
    "plt.ylabel(\"to_poi_frac\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While no significant trend was seen, the red points (poi) and blue points (non-poi) are slightly more distinctly separated into clusters which could potentially contribute to training the algorithm. The effect/importance of each feature including these are examined later.\n",
    "\n",
    "As such, the new list of features to consider would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total_stock_value', 'total_payments', 'restricted_stock', 'exercised_stock_options', 'salary', 'expenses', 'shared_receipt_with_poi', 'bonus', 'from_poi_frac', 'to_poi_frac']\n",
      "Total: 10 features\n"
     ]
    }
   ],
   "source": [
    "cleaned_feature_list.remove(\"from_poi_to_this_person\")\n",
    "cleaned_feature_list.remove(\"to_messages\")\n",
    "cleaned_feature_list.remove(\"from_this_person_to_poi\")\n",
    "cleaned_feature_list.remove(\"from_messages\")\n",
    "\n",
    "cleaned_feature_list.append(\"from_poi_frac\")\n",
    "cleaned_feature_list.append(\"to_poi_frac\")\n",
    "\n",
    "print cleaned_feature_list\n",
    "print \"Total:\", len(cleaned_feature_list), \"features\"\n",
    "\n",
    "# New features created are part of the feature list as seen below (Udacity requirement - Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature selection</h3>\n",
    "\n",
    "With the 10 features to be considered for training, the next step was to figure out whether to use all 10 features for training, or just a subset and if the latter, which features to select. \n",
    "\n",
    "To find the optimal selection, 8 different selections of features were made using sklearn's feature selection tool where k took on values from 3-10. k refers to the parameter in SelectKBest function, which selects the best k features from a given list of features. So, after the loop was run, there were 8 sets of features with the first set having the best 3 features, next having best 4 features and so on till the 8th set which had all 10 features. Within each these feature selections, the data was split into training and test data. \n",
    "\n",
    "In the next section, \"Algorithms\", different algos were trained using the training data and then evaluated using the test data for each of the 8 feature sets. In this way, it was possible to simultaneously identify the best selection of features as well as the best training algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_list below is input for Udacity's starter code.\n",
    "# It is a list of strings, each of which is a feature name. The first feature must be \"poi\" (label).\n",
    "\n",
    "features_list = ['poi','salary', 'bonus', 'total_stock_value', 'total_payments', 'restricted_stock', \\\n",
    "                 'exercised_stock_options', 'expenses', 'shared_receipt_with_poi', 'from_poi_frac', 'to_poi_frac']\n",
    "\n",
    "\n",
    "# Store to \"my_dataset\" for easy export below. \"my_dataset\" is input for Udacity's starter code.\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "# featureFormat, targetFeatureSplit are functions from Udacity's starter code. As mentioned under Project Requirements\n",
    "# (in README.md), these functions take in the dataset and the features chosen, and puts them into a numpy array \n",
    "# suitable for input for sklearn functions. \n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop to create all 8 features sets ie. k = 3,4,5...10\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "new_features_list = []\n",
    "feature_score_dict = {}\n",
    "\n",
    "for i in range(3,11):\n",
    "    \n",
    "    selector = SelectKBest(f_classif, k=i)\n",
    "    selector.fit(features, labels)\n",
    "\n",
    "    new_features = selector.transform(features)\n",
    "    \n",
    "    new_features_list.append(new_features)\n",
    "    \n",
    "    # To create a reverse sorted list of tuples of format, (feature, score), to identify the features being used in\n",
    "    # the various algos later.\n",
    "    if i==10:\n",
    "        scores = selector.scores_\n",
    "        features_list.pop(0)\n",
    "        \n",
    "        for j in range(len(scores)):\n",
    "            feature_score_dict[features_list[j]] = scores[j]\n",
    "        \n",
    "        feature_score_tup_sorted = sorted(feature_score_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each feature set, create training + test features and training + test labels\n",
    "# They are stored in separate lists for easy traversal during training\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train_list = []\n",
    "features_test_list = []\n",
    "labels_train_list = []\n",
    "labels_test_list = []\n",
    "\n",
    "for features in new_features_list:\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    features_train_list.append(features_train)\n",
    "    features_test_list.append(features_test)\n",
    "    labels_train_list.append(labels_train)\n",
    "    labels_test_list.append(labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of sets of training features: 8 \n",
      "\n",
      "No. of features in each data point in 1st set: 3\n",
      "1st set, 1st data point: [ 800000.  495633.  117551.] \n",
      "\n",
      "No. of features in each data point in last set: 10\n",
      "last set, 1st data point: [  3.14288000e+05   8.00000000e+05   4.95633000e+05   1.10139300e+06\n",
      "   3.78082000e+05   1.17551000e+05   2.78610000e+04   7.10000000e+01\n",
      "   0.00000000e+00   1.21212121e-01]\n"
     ]
    }
   ],
   "source": [
    "# For reference and checking\n",
    "\n",
    "print \"No of sets of training features:\", len(features_train_list), \"\\n\"\n",
    "print \"No. of features in each data point in 1st set:\", len(features_train_list[0][0])\n",
    "print \"1st set, 1st data point:\", features_train_list[0][0], \"\\n\"\n",
    "print \"No. of features in each data point in last set:\", len(features_train_list[7][0])\n",
    "print \"last set, 1st data point:\", features_train_list[7][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Common Functions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for evaluation of algo. \n",
    "# Evaluation is primarily based on precicision score and recall score.\n",
    "# Function will print \"Accept\" if both scores are above 0.3 (udacity's requirement)\n",
    "\n",
    "def evaluate(labels_test, pred):\n",
    "    acc = accuracy_score(labels_test, pred)\n",
    "    ps = precision_score(labels_test, pred)\n",
    "    rs = recall_score(labels_test, pred)\n",
    "\n",
    "    print \"acc:\", acc\n",
    "    print \"precision_score:\", ps\n",
    "    print \"recall_score:\", rs\n",
    "\n",
    "    # Evaluation criteria\n",
    "    print \"\\n\"\n",
    "    if ps < 0.3 or rs < 0.3:\n",
    "        print \"Reject, precision and/or recall score < 0.3\"\n",
    "    else:\n",
    "        print \"Accept\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Decision Tree</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'min_samples_split': 5, 'criterion': 'entropy'}\n",
      "k: 3\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.333333333333\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 5, 'criterion': 'entropy'}\n",
      "k: 4\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.333333333333\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 6, 'criterion': 'entropy'}\n",
      "k: 5\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.8\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 9, 'criterion': 'gini'}\n",
      "k: 6\n",
      "acc: 0.795454545455\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 7, 'criterion': 'entropy'}\n",
      "k: 7\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 9, 'criterion': 'gini'}\n",
      "k: 8\n",
      "acc: 0.818181818182\n",
      "precision_score: 0.285714285714\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 9, 'criterion': 'gini'}\n",
      "k: 9\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.25\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "parameters: {'min_samples_split': 9, 'criterion': 'gini'}\n",
      "k: 10\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.25\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The decision tree algo was run for each of the set of features. Within each, the paramaters are tuned and optimized\n",
    "# using GridSearchCV. All output is shown below.\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "for i in range(len(features_train_list)):\n",
    "    params = {'min_samples_split':[2,3,4,5,6,7,8,9,10], 'criterion':('gini', 'entropy')}\n",
    "    clf_tree = tree.DecisionTreeClassifier()\n",
    "    clf = GridSearchCV(clf_tree, params)\n",
    "    \n",
    "    features_train = features_train_list[i]\n",
    "    features_test = features_test_list[i]\n",
    "    labels_train = labels_train_list[i]\n",
    "    labels_test = labels_test_list[i]\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    print \"parameters:\", clf.best_params_\n",
    "\n",
    "    pred = clf.predict(features_test)\n",
    "    \n",
    "    print \"k:\", len(features_train[0])\n",
    "    evaluate(labels_test, pred)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.6\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 4\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.6\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 5\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.6\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 6\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.6\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 7\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 8\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 9\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "k: 10\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The naive bayes algo was run for each of the set of features. All output is shown below.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "for i in range(len(features_train_list)):\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    \n",
    "    features_train = features_train_list[i]\n",
    "    features_test = features_test_list[i]\n",
    "    labels_train = labels_train_list[i]\n",
    "    labels_test = labels_test_list[i]\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    pred = clf.predict(features_test)\n",
    "    \n",
    "    print \"k:\", len(features_train[0])\n",
    "    evaluate(labels_test, pred)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>K Nearest Neighbors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3}\n",
      "k: 3\n",
      "acc: 0.863636363636\n",
      "precision_score: 0.333333333333\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "{'n_neighbors': 2}\n",
      "k: 4\n",
      "acc: 0.863636363636\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "{'n_neighbors': 2}\n",
      "k: 5\n",
      "acc: 0.863636363636\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "k: 6\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.25\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "k: 7\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.666666666667\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "k: 8\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.666666666667\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "k: 9\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.666666666667\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n",
      "{'n_neighbors': 3}\n",
      "k: 10\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.666666666667\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The K nearest neighbor algo was run for each of the set of features. Within each, the paramaters are tuned \n",
    "# and optimized using GridSearchCV. All output is shown below.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for i in range(len(features_train_list)):\n",
    "    params = {'n_neighbors':[2,3,4,5,6,7,8,9,10]}\n",
    "    clf_knn = KNeighborsClassifier()\n",
    "    clf = GridSearchCV(clf_knn, params)\n",
    "    \n",
    "    features_train = features_train_list[i]\n",
    "    features_test = features_test_list[i]\n",
    "    labels_train = labels_train_list[i]\n",
    "    labels_test = labels_test_list[i]\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    print clf.best_params_\n",
    "\n",
    "    pred = clf.predict(features_test)\n",
    "    \n",
    "    print \"k:\", len(features_train[0])\n",
    "    evaluate(labels_test, pred)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>AdaBoost</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 3\n",
      "acc: 0.795454545455\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 4\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 5\n",
      "acc: 0.840909090909\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 6\n",
      "acc: 0.795454545455\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 7\n",
      "acc: 0.795454545455\n",
      "precision_score: 0.166666666667\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 8\n",
      "acc: 0.818181818182\n",
      "precision_score: 0.2\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 9\n",
      "acc: 0.818181818182\n",
      "precision_score: 0.2\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n",
      "k: 10\n",
      "acc: 0.795454545455\n",
      "precision_score: 0.25\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The AdaBoost algo was run for each of the set of features. All output is shown below.\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for i in range(len(features_train_list)):\n",
    "    \n",
    "    clf = AdaBoostClassifier()\n",
    "    \n",
    "    features_train = features_train_list[i]\n",
    "    features_test = features_test_list[i]\n",
    "    labels_train = labels_train_list[i]\n",
    "    labels_test = labels_test_list[i]\n",
    "\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    pred = clf.predict(features_test)\n",
    "    \n",
    "    print \"k:\", len(features_train[0])\n",
    "    evaluate(labels_test, pred)\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Support Vector Machine</h3>\n",
    "\n",
    "SVM was implemented separately from the algos above as it required an additional step of feature scaling. Additionally, after some testing, it was also found that with more than 4 features, the result was not at all desirable ie. recall and precision scores were 0. So, SVM was not run using the 8 sets of features from above; only with set of 3 and 4 features. Best performing scenario (k=3) is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'C': 10, 'gamma': 100}\n",
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.2\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Since the scale of data varies significantly across the various features, feature scaling is required.\n",
    "scaler = MinMaxScaler()\n",
    "rescaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# After rescaling, feature selection was implemented.\n",
    "selector = SelectKBest(f_classif, k=3)\n",
    "selector.fit(rescaled_features, labels)\n",
    "new_rescaled_features = selector.transform(rescaled_features)\n",
    "\n",
    "# After selecting the best features, they were split into training and testing features\n",
    "new_rescaled_features_train, new_rescaled_features_test, labels_train, labels_test = \\\n",
    "    train_test_split(new_rescaled_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVM algo\n",
    "params = {'kernel':('linear','rbf'), 'C':[10,100,1000,10000], 'gamma':[10,100,1000,10000]}\n",
    "clf_svc = SVC()\n",
    "clf = GridSearchCV(clf_svc, params)\n",
    "clf.fit(new_rescaled_features_train, labels_train)\n",
    "\n",
    "print clf.best_params_\n",
    "\n",
    "pred = clf.predict(new_rescaled_features_test)\n",
    "\n",
    "evaluate(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Principal Component Analysis (PCA)</h3>\n",
    "\n",
    "Implemented PCA to reduce feature dimensionality, identify potential latent features (subset of features that drive the patterns in data) and hopefully improve performance of algorithms. \"n_components\" parameter in PCA was set to be 4 (other values were tested but this provided best results). The alogos used were Decision Tree, Naive Bayes, K Nearest Neighbours and AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance value (eigenvalue) for each principal component:\n",
      "[ 0.88519242  0.09854918  0.01223282  0.00383618] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=4)\n",
    "\n",
    "# \"features\" being used here is prior to any feature selection done above. So, it contains all 10 feature data\n",
    "# and has not been split into training or test data yet. \n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "pca.fit(features_train)\n",
    "\n",
    "trans_features_train = pca.transform(features_train)\n",
    "trans_features_test = pca.transform(features_test)\n",
    "\n",
    "print \"Importance value (eigenvalue) for each principal component:\"\n",
    "print pca.explained_variance_ratio_, \"\\n\"\n",
    "\n",
    "# To view the breakdown of each component. Commented out as its not necessary for the project, only for reference\n",
    "# print \"first pc\", pca.components_[0]\n",
    "# print \"second pc\", pca.components_[1]\n",
    "# print \"third pc\", pca.components_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'min_samples_split': 10, 'criterion': 'entropy'}\n",
      "acc: 0.772727272727\n",
      "precision_score: 0.0\n",
      "recall_score: 0.0\n",
      "\n",
      "\n",
      "Reject, precision and/or recall score < 0.3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "params = {'min_samples_split':[2,3,4,5,6,7,8,9,10], 'criterion':('gini', 'entropy')}\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf = GridSearchCV(clf_tree, params)\n",
    "\n",
    "clf.fit(trans_features_train, labels_train)\n",
    "\n",
    "print \"parameters:\", clf.best_params_\n",
    "\n",
    "pred = clf.predict(trans_features_test)\n",
    "\n",
    "evaluate(labels_test, pred)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3}\n",
      "acc: 0.909090909091\n",
      "precision_score: 0.666666666667\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K Nearest Neighbours\n",
    "\n",
    "params = {'n_neighbors':[2,3,4,5,6,7,8,9,10]}\n",
    "clf_knn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(clf_knn, params)\n",
    "\n",
    "clf.fit(trans_features_train, labels_train)\n",
    "\n",
    "print clf.best_params_\n",
    "\n",
    "pred = clf.predict(trans_features_test)\n",
    "\n",
    "evaluate(labels_test, pred)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.886363636364\n",
      "precision_score: 0.5\n",
      "recall_score: 0.6\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "clf.fit(trans_features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(trans_features_test)\n",
    "\n",
    "evaluate(labels_test, pred)\n",
    "print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.840909090909\n",
      "precision_score: 0.333333333333\n",
      "recall_score: 0.4\n",
      "\n",
      "\n",
      "Accept\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "clf.fit(trans_features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(trans_features_test)\n",
    "\n",
    "evaluate(labels_test, pred)\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Conclusion\n",
    "\n",
    "<h3>Algorithm Performance</h3>\n",
    "Shown below are the algorithms and the corresponding parameter configurations that produced the best results (ie. highest precision and recall score). Some of the algorithms actually gave the same scores for different values of k (no. of features). Since it is always better to obtain the same result with less data (ie. less features), in such cases, the algorithm and parameter tunings with lowest k was selected. \n",
    "\n",
    "algorithm: Decision Tree\n",
    "<br>\n",
    "parameters: {'min_samples_split': 6, 'criterion': 'entropy'}\n",
    "<br>\n",
    "k: 5\n",
    "<br>\n",
    "acc: 0.886363636364\n",
    "<br>\n",
    "precision_score: 0.5\n",
    "<br>\n",
    "recall_score: 0.8\n",
    "<br>\n",
    "<br>\n",
    "algorithm: Naive Bayes\n",
    "<br>\n",
    "k: 3\n",
    "<br>\n",
    "acc: 0.909090909091\n",
    "<br>\n",
    "precision_score: 0.6\n",
    "<br>\n",
    "recall_score: 0.6\n",
    "<br>\n",
    "<br>\n",
    "algorithm: K Nearest Neighbor\n",
    "<br>\n",
    "{'n_neighbors': 3}\n",
    "<br>\n",
    "k: 7\n",
    "<br>\n",
    "acc: 0.909090909091\n",
    "<br>\n",
    "precision_score: 0.666666666667\n",
    "<br>\n",
    "recall_score: 0.4\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Choosing Algorithm</h3>\n",
    "Having identified the top 3 performing algorithms and configurations, Udacity's tester.py script was used to select the best among them. The script does k-fold cross validation and evaluates the classifier. It was not used for evaluating all possible cases of the various algorithms above mainly because compute time and power required would increase significantly. As such, basic evaluation was first done to select the top 3 algos before running Udacity's script.\n",
    "<br>\n",
    "Note: test.py was run for all 3 algos, but only the best performing case is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters chosen:\n",
      "['exercised_stock_options', 'total_stock_value', 'bonus']\n"
     ]
    }
   ],
   "source": [
    "# Identify the features used in the algos. \n",
    "final_features_list = []\n",
    "\n",
    "# for Decision tree - range(5), Naive Bayes - range(3), K Nearest Neighbors - range(7)\n",
    "for i in range(3):\n",
    "    final_features_list.append(feature_score_tup_sorted[i][0])\n",
    "    \n",
    "print \"Parameters chosen:\"\n",
    "print final_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dump classifier, dataset and feature_list into .pkl files which are input for tester.py code.\n",
    "\n",
    "final_features_list.insert(0,\"poi\")\n",
    "\n",
    "data = featureFormat(my_dataset, final_features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# create training and testing features based on chosen features\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# create clf with chosen algo and parameter config - only best performing case is uncommented below\n",
    "\n",
    "#clf = tree.DecisionTreeClassifier(min_samples_split=6, criterion='entropy')\n",
    "clf = GaussianNB()\n",
    "#clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# dump into .pkl files\n",
    "dump_classifier_and_data(clf, my_dataset, final_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Output from tester.py</h3>\n",
    "<br>\n",
    "<b>1. </b>DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=6, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best')\n",
    "<br>\n",
    "<br>\n",
    "Accuracy: 0.80529\tPrecision: 0.32747\tRecall: 0.34450\tF1: 0.33577\tF2: 0.34095\n",
    "<br>\n",
    "<br>\n",
    "Total predictions: 14000\n",
    "<br>\n",
    "<br>\n",
    "True positives:  689\tFalse positives: 1415\tFalse negatives: 1311\tTrue negatives: 10585\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>2. </b>GaussianNB(priors=None)\n",
    "<br>\n",
    "<br>\n",
    "Accuracy: 0.84300\tPrecision: 0.48581\tRecall: 0.35100\tF1: 0.40755\tF2: 0.37163\n",
    "<br>\n",
    "<br>\n",
    "Total predictions: 13000\n",
    "<br>\n",
    "<br>\n",
    "True positives:  702\tFalse positives:  743\tFalse negatives: 1298\tTrue negatives: 10257\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>3. </b>KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=3, p=2,weights='uniform')\n",
    "<br>\n",
    "<br>\n",
    "Accuracy: 0.86720\tPrecision: 0.50402\tRecall: 0.25100\tF1: 0.33511\tF2: 0.27901\n",
    "<br>\n",
    "<br>\n",
    "Total predictions: 15000\n",
    "<br>\n",
    "<br>\n",
    "True positives:  502\tFalse positives:  494\tFalse negatives: 1498\tTrue negatives: 12506\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "Note: The recall and precision scores initially obtained are different from the tester.py values because the latter uses k-fold cross validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Project\n",
    "\n",
    "Based on the results above, Gaussian Naive Bayes with 3 training features is the best performing algorithm and is the final algo chosen for this project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
